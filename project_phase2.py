# -*- coding: utf-8 -*-
"""Project_Phase2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e21EU3snPmAoPQLNK5JbPyxzK63z-a-s

Upload the Dataset
"""

from google.colab import files
uploaded = files.upload()

""" Load the Dataset

"""

import pandas as pd

# Replace with your dataset path
dataset_path = "customer_support_chat_data.csv"

try:
    # Load the dataset
    df = pd.read_csv(dataset_path)

    # Display the first few rows
    print("Dataset loaded successfully!\n")
    print(df.head())

    # Show basic information
    print("\nDataset Information:")
    print(df.info())

except FileNotFoundError:
    print(f"Error: The file at '{dataset_path}' was not found.")
except Exception as e:
    print(f"An error occurred: {e}")

"""Data Exploration

"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Replace with the path to your dataset
dataset_path = "customer_support_chat_data.csv"

# Load the dataset
try:
    df = pd.read_csv(dataset_path)

    print("Dataset loaded successfully!\n")
    print(df.head())

    # Basic info: checking for data types and missing values
    print("\nDataset Information:")
    print(df.info())

    # Check for missing values
    print("\nMissing Values:")
    print(df.isnull().sum())

    # Summary statistics for numerical columns
    print("\nSummary Statistics:")
    print(df.describe())

    # Exploring the distribution of query lengths
    df['query_length'] = df['Customer_Query'].apply(lambda x: len(str(x)))

    plt.figure(figsize=(10, 6))
    sns.histplot(df['query_length'], kde=True, bins=30, color='skyblue')
    plt.title('Distribution of Customer Query Lengths')
    plt.xlabel('Query Length')
    plt.ylabel('Frequency')
    plt.show()

    # Exploring the distribution of response lengths
    df['response_length'] = df['Bot_Response'].apply(lambda x: len(str(x)))

    plt.figure(figsize=(10, 6))
    sns.histplot(df['response_length'], kde=True, bins=30, color='lightcoral')
    plt.title('Distribution of Bot Response Lengths')
    plt.xlabel('Response Length')
    plt.ylabel('Frequency')
    plt.show()

    # If you have a sentiment or intent column, you can explore the distribution of these labels:
    if 'Sentiment' in df.columns:
        plt.figure(figsize=(8, 6))
        sns.countplot(data=df, x='Sentiment', palette='viridis')
        plt.title('Distribution of Sentiment in Bot Responses')
        plt.xlabel('Sentiment')
        plt.ylabel('Frequency')
        plt.show()

    # You can also visualize the most frequent customer queries (Word Cloud or Top Queries)
    from collections import Counter
    from wordcloud import WordCloud

    # Combine all customer queries into one string
    all_queries = ' '.join(df['Customer_Query'].dropna().astype(str))

    # Generate a WordCloud for most common words
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_queries)

    plt.figure(figsize=(10, 6))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title('Most Frequent Words in Customer Queries')
    plt.show()

except FileNotFoundError:
    print(f"Error: The file at '{dataset_path}' was not found.")
except Exception as e:
    print(f"An error occurred: {e}")

"""Check for Missing Values and Duplicates

"""

import pandas as pd

# Replace with the path to your dataset
dataset_path = "customer_support_chat_data.csv"

# Load the dataset
try:
    df = pd.read_csv(dataset_path)

    print("Dataset loaded successfully!\n")

    # Check for missing values
    print("\nChecking for missing values...\n")
    missing_values = df.isnull().sum()
    print(missing_values[missing_values > 0])  # Display only columns with missing values

    # Check for duplicate rows
    print("\nChecking for duplicate rows...\n")
    duplicate_rows = df.duplicated().sum()  # Count duplicate rows
    print(f"Number of duplicate rows: {duplicate_rows}")

    # If duplicates exist, show the first few duplicates
    if duplicate_rows > 0:
        print("\nFirst few duplicate rows:")
        print(df[df.duplicated()].head())

    # Handle missing values (you can either drop or fill them)
    # Example 1: Drop rows with missing values
    df_no_missing = df.dropna()
    print(f"\nDataset size after dropping missing values: {df_no_missing.shape}")

    # Example 2: Fill missing values with a default value (like 'unknown' or mean/median)
    df_filled = df.fillna({"Customer_Query": "Unknown", "Bot_Response": "No Response"})
    print("\nMissing values have been filled.\n")

    # Show the cleaned dataset (after handling missing values)
    print("\nCleaned Dataset after Handling Missing Values:")
    print(df_filled.head())

    # Handling duplicates: You can drop duplicates
    df_no_duplicates = df.drop_duplicates()
    print(f"\nDataset size after dropping duplicates: {df_no_duplicates.shape}")

    # Show the cleaned dataset (after handling duplicates)
    print("\nCleaned Dataset after Removing Duplicates:")
    print(df_no_duplicates.head())

except FileNotFoundError:
    print(f"Error: The file at '{dataset_path}' was not found.")
except Exception as e:
    print(f"An error occurred: {e}")

"""Visualize a Few Features

"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from collections import Counter

# Replace with the path to your dataset
dataset_path = "customer_support_chat_data.csv"

# Load the dataset
try:
    df = pd.read_csv(dataset_path)

    # Ensure the data is loaded correctly
    print("Dataset loaded successfully!\n")
    print(df.head())

    # 1. Query Length vs Bot Response Length
    df['query_length'] = df['Customer_Query'].apply(lambda x: len(str(x)))
    df['response_length'] = df['Bot_Response'].apply(lambda x: len(str(x)))

    plt.figure(figsize=(10, 6))
    sns.scatterplot(x='query_length', y='response_length', data=df, color='skyblue')
    plt.title('Customer Query Length vs Bot Response Length')
    plt.xlabel('Customer Query Length')
    plt.ylabel('Bot Response Length')
    plt.show()

    # 2. Distribution of Query Lengths (Histogram)
    plt.figure(figsize=(10, 6))
    sns.histplot(df['query_length'], kde=True, bins=30, color='lightcoral')
    plt.title('Distribution of Customer Query Lengths')
    plt.xlabel('Query Length')
    plt.ylabel('Frequency')
    plt.show()

    # 3. Distribution of Response Lengths (Histogram)
    plt.figure(figsize=(10, 6))
    sns.histplot(df['response_length'], kde=True, bins=30, color='lightgreen')
    plt.title('Distribution of Bot Response Lengths')
    plt.xlabel('Response Length')
    plt.ylabel('Frequency')
    plt.show()

    # 4. Most Frequent Words in Customer Queries (Word Cloud)
    all_queries = ' '.join(df['Customer_Query'].dropna().astype(str))
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_queries)

    plt.figure(figsize=(10, 6))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title('Most Frequent Words in Customer Queries')
    plt.show()

    # 5. Distribution of Sentiment (If Sentiment Column Exists)
    if 'Sentiment' in df.columns:
        plt.figure(figsize=(8, 6))
        sns.countplot(data=df, x='Sentiment', palette='viridis')
        plt.title('Distribution of Sentiment in Customer Queries')
        plt.xlabel('Sentiment')
        plt.ylabel('Frequency')
        plt.show()

    # 6. Distribution of Bot Response Categories (If Intent Exists)
    if 'Intent' in df.columns:
        plt.figure(figsize=(10, 6))
        sns.countplot(data=df, x='Intent', palette='magma')
        plt.title('Distribution of Bot Response Intents')
        plt.xlabel('Intent')
        plt.ylabel('Frequency')
        plt.xticks(rotation=45)
        plt.show()

except FileNotFoundError:
    print(f"Error: The file at '{dataset_path}' was not found.")
except Exception as e:
    print(f"An error occurred: {e}")

""" Identify Target and Features

"""

import spacy
from spacy import displacy
import re

# Load the large English model from spaCy
nlp = spacy.load("en_core_web_sm")

# Example text: "Revolutionizing Customer Support with an Intelligent Chatbot"
text = """
Revolutionizing Customer Support with an Intelligent Chatbot

In today's fast-paced world, businesses must provide quick and efficient customer support. Traditional support channels like phone calls and emails are becoming outdated, and customers expect faster responses.

An intelligent chatbot powered by artificial intelligence (AI) offers a solution. These chatbots are capable of understanding customer queries, offering real-time solutions, and providing 24/7 support. They can be integrated into websites, mobile apps, and social media platforms.

The main features of an intelligent chatbot include natural language processing (NLP) for understanding customer queries, machine learning for continuous improvement, and data-driven insights for personalized customer service.

Target Audience:
1. Businesses of all sizes looking to automate customer support.
2. Customers who prefer fast, self-service support options.
3. Support agents who benefit from AI-driven tools for handling more complex cases.

"""

# Process the text through the NLP pipeline
doc = nlp(text)

# Function to extract targets (audience)
def extract_target(doc):
    targets = []
    for sent in doc.sents:
        if "target" in sent.text.lower() or "audience" in sent.text.lower():
            targets.append(sent.text)
    return targets

# Function to extract features of the chatbot
def extract_features(doc):
    features = []
    for sent in doc.sents:
        if "feature" in sent.text.lower() or "capability" in sent.text.lower() or "functionality" in sent.text.lower():
            features.append(sent.text)
    return features

# Function to identify key phrases related to the chatbot and customer support
def extract_keywords(doc):
    keywords = []
    for token in doc:
        if token.pos_ in ["NOUN", "ADJ", "PROPN"]:
            keywords.append(token.text.lower())
    return list(set(keywords))

# Extracting information
target = extract_target(doc)
features = extract_features(doc)
keywords = extract_keywords(doc)

# Output the results
print("Targets Identified:")
for t in target:
    print(t)

print("\nFeatures Identified:")
for f in features:
    print(f)

print("\nKeywords Identified:")
print(keywords)

"""Convert Categorical Columns to Numerical

"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Sample dataset
data = {
    'User ID': [1, 2, 3, 4],
    'Customer\'s Issue Type': ['Technical Issue', 'Billing Issue', 'Technical Issue', 'Account Issue'],
    'Chatbot Model': ['AI-Chatbot', 'Rule-Based', 'AI-Chatbot', 'Hybrid'],
    'User Satisfaction': ['High', 'Low', 'Medium', 'High']
}

# Create DataFrame
df = pd.DataFrame(data)

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Convert 'Customer\'s Issue Type' using Label Encoding
df['Customer\'s Issue Type'] = label_encoder.fit_transform(df['Customer\'s Issue Type'])

# Convert 'Chatbot Model' using Label Encoding
df['Chatbot Model'] = label_encoder.fit_transform(df['Chatbot Model'])

# Convert 'User Satisfaction' using Label Encoding
df['User Satisfaction'] = label_encoder.fit_transform(df['User Satisfaction'])

# Display the result
print("Converted Dataset:")
print(df)

"""One-Hot Encoding

"""

import pandas as pd

# Sample dataset
data = {
    'User ID': [1, 2, 3, 4],
    'Customer\'s Issue Type': ['Technical Issue', 'Billing Issue', 'Technical Issue', 'Account Issue'],
    'Chatbot Model': ['AI-Chatbot', 'Rule-Based', 'AI-Chatbot', 'Hybrid'],
    'User Satisfaction': ['High', 'Low', 'Medium', 'High']
}

# Create DataFrame
df = pd.DataFrame(data)

# Apply One-Hot Encoding to categorical columns
df_one_hot = pd.get_dummies(df, columns=['Customer\'s Issue Type', 'Chatbot Model', 'User Satisfaction'])

# Display the result
print("One-Hot Encoded Dataset:")
print(df_one_hot)

"""Feature Scaling

"""

import pandas as pd
from sklearn.preprocessing import MinMaxScaler, StandardScaler

# Sample dataset
data = {
    'User ID': [1, 2, 3, 4],
    'Response Time (sec)': [34, 120, 60, 90],
    'Customer Satisfaction Score': [7, 8, 5, 9],
    'Chatbot Interactions': [15, 45, 30, 60]
}

# Create DataFrame
df = pd.DataFrame(data)

# Extract numerical features to scale
numerical_features = ['Response Time (sec)', 'Customer Satisfaction Score', 'Chatbot Interactions']

# Min-Max Scaling
min_max_scaler = MinMaxScaler()
df_min_max_scaled = df.copy()  # Copy the original data to avoid modifying it directly
df_min_max_scaled[numerical_features] = min_max_scaler.fit_transform(df[numerical_features])

# Standardization (Z-score Scaling)
standard_scaler = StandardScaler()
df_standard_scaled = df.copy()  # Copy the original data to avoid modifying it directly
df_standard_scaled[numerical_features] = standard_scaler.fit_transform(df[numerical_features])

# Display the results
print("Original Dataset:")
print(df)

print("\nMin-Max Scaled Dataset:")
print(df_min_max_scaled)

print("\nStandardized Dataset (Z-score):")
print(df_standard_scaled)

"""Train-Test Split

"""

import pandas as pd
from sklearn.model_selection import train_test_split

# Sample dataset
data = {
    'Response Time (sec)': [34, 120, 60, 90, 45, 30, 100, 70],
    'Customer Satisfaction Score': [7, 8, 5, 9, 6, 7, 8, 6],
    'Chatbot Interactions': [15, 45, 30, 60, 50, 20, 80, 55],
    'Customer Satisfaction (Target)': [1, 0, 0, 1, 1, 0, 1, 1]  # 1 = Satisfied, 0 = Not Satisfied
}

# Create DataFrame
df = pd.DataFrame(data)

# Features (X) and Target (y)
X = df[['Response Time (sec)', 'Customer Satisfaction Score', 'Chatbot Interactions']]
y = df['Customer Satisfaction (Target)']

# Perform Train-Test Split (80% Train, 20% Test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the results
print("Training Data (X_train):")
print(X_train)

print("\nTest Data (X_test):")
print(X_test)

print("\nTraining Labels (y_train):")
print(y_train)

print("\nTest Labels (y_test):")
print(y_test)

"""Model Building

"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Sample dataset
data = {
    'Response Time (sec)': [34, 120, 60, 90, 45, 30, 100, 70],
    'Customer Satisfaction Score': [7, 8, 5, 9, 6, 7, 8, 6],
    'Chatbot Interactions': [15, 45, 30, 60, 50, 20, 80, 55],
    'Customer Satisfaction (Target)': [1, 0, 0, 1, 1, 0, 1, 1]  # 1 = Satisfied, 0 = Not Satisfied
}

# Create DataFrame
df = pd.DataFrame(data)

# Features (X) and Target (y)
X = df[['Response Time (sec)', 'Customer Satisfaction Score', 'Chatbot Interactions']]
y = df['Customer Satisfaction (Target)']

# Train-Test Split (80% Train, 20% Test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature Scaling (Standardization)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize Logistic Regression Model
model = LogisticRegression()

# Train the model
model.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test_scaled)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

"""Evaluation

"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Sample dataset
data = {
    'Response Time (sec)': [34, 120, 60, 90, 45, 30, 100, 70],
    'Customer Satisfaction Score': [7, 8, 5, 9, 6, 7, 8, 6],
    'Chatbot Interactions': [15, 45, 30, 60, 50, 20, 80, 55],
    'Customer Satisfaction (Target)': [1, 0, 0, 1, 1, 0, 1, 1]  # 1 = Satisfied, 0 = Not Satisfied
}

# Create DataFrame
df = pd.DataFrame(data)

# Features (X) and Target (y)
X = df[['Response Time (sec)', 'Customer Satisfaction Score', 'Chatbot Interactions']]
y = df['Customer Satisfaction (Target)']

# Train-Test Split (80% Train, 20% Test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature Scaling (Standardization)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize Logistic Regression Model
model = LogisticRegression()

# Train the model
model.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test_scaled)

# Evaluate the model

# 1. Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")

# 2. Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# 3. Confusion Matrix
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Optionally, display metrics individually
print("\nPrecision, Recall, and F1-Score for Class 1 (Satisfied):")
print(f"Precision: {classification_report(y_test, y_pred, output_dict=True)['1']['precision']}")
print(f"Recall: {classification_report(y_test, y_pred, output_dict=True)['1']['recall']}")
print(f"F1-Score: {classification_report(y_test, y_pred, output_dict=True)['1']['f1-score']}")

"""Make Predictions from New Input"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

# Sample dataset (same as before)
data = {
    'Response Time (sec)': [34, 120, 60, 90, 45, 30, 100, 70],
    'Customer Satisfaction Score': [7, 8, 5, 9, 6, 7, 8, 6],
    'Chatbot Interactions': [15, 45, 30, 60, 50, 20, 80, 55],
    'Customer Satisfaction (Target)': [1, 0, 0, 1, 1, 0, 1, 1]  # 1 = Satisfied, 0 = Not Satisfied
}

# Create DataFrame
df = pd.DataFrame(data)

# Features (X) and Target (y)
X = df[['Response Time (sec)', 'Customer Satisfaction Score', 'Chatbot Interactions']]
y = df['Customer Satisfaction (Target)']

# Train-Test Split (80% Train, 20% Test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature Scaling (Standardization)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize and Train Logistic Regression Model
model = LogisticRegression()
model.fit(X_train_scaled, y_train)

# New input data (new customer)
new_data = {
    'Response Time (sec)': [50],  # Example value for new customer
    'Customer Satisfaction Score': [7],  # Example value
    'Chatbot Interactions': [40]  # Example value
}

# Create DataFrame for the new input
new_input = pd.DataFrame(new_data)

# Apply the same scaling to the new input data
new_input_scaled = scaler.transform(new_input)

# Make prediction with the trained model
prediction = model.predict(new_input_scaled)

# Output the prediction
if prediction[0] == 1:
    print("Prediction: Customer is Satisfied")
else:
    print("Prediction: Customer is Not Satisfied")

"""Convert to DataFrame and Encode"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler

# Sample raw data (you may get this data from CSV, database, etc.)
data = {
    'Customer Type': ['New', 'Returning', 'New', 'Returning', 'New'],
    'Satisfaction Level': ['High', 'Low', 'Medium', 'High', 'Medium'],
    'Response Time (sec)': [34, 120, 60, 90, 45],
    'Customer Satisfaction (Target)': [1, 0, 0, 1, 1]  # 1 = Satisfied, 0 = Not Satisfied
}

# Convert to DataFrame
df = pd.DataFrame(data)

# Display the original DataFrame
print("Original DataFrame:")
print(df)

# Encode categorical variables (Customer Type and Satisfaction Level)
# 1. Using LabelEncoder for "Customer Type" and "Satisfaction Level"
label_encoder = LabelEncoder()

# Encode "Customer Type"
df['Customer Type Encoded'] = label_encoder.fit_transform(df['Customer Type'])

# Encode "Satisfaction Level"
df['Satisfaction Level Encoded'] = label_encoder.fit_transform(df['Satisfaction Level'])

# 2. Using OneHotEncoder (for illustration, we will use OneHotEncoding on "Satisfaction Level")
# OneHotEncoder will create separate columns for each category in the "Satisfaction Level"
column_transformer = ColumnTransformer(
    transformers=[
        ('encoder', OneHotEncoder(), ['Satisfaction Level'])
    ],
    remainder='passthrough'  # Keep other columns as they are
)

# Apply OneHotEncoder transformation
transformed_data = column_transformer.fit_transform(df)

# Get feature names after OneHotEncoding
feature_names = column_transformer.get_feature_names_out(df.columns)

# Create DataFrame with correct column names
df_encoded = pd.DataFrame(transformed_data, columns=feature_names)

# Display the final DataFrame with encoded features
print("\nDataFrame with Encoded Features:")
print(df_encoded)

"""Predict the Final Grade"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

# Sample dataset: customer interactions data
data = {
    'Response Time (sec)': [34, 120, 60, 90, 45, 30, 100, 70],
    'Customer Satisfaction Score': [7, 8, 5, 9, 6, 7, 8, 6],
    'Chatbot Interactions': [15, 45, 30, 60, 50, 20, 80, 55],
    'Final Grade': [85, 60, 70, 95, 80, 72, 90, 75]  # Target: Final Grade (score)
}

# Convert to DataFrame
df = pd.DataFrame(data)

# Features (X) and Target (y)
X = df[['Response Time (sec)', 'Customer Satisfaction Score', 'Chatbot Interactions']]
y = df['Final Grade']

# Train-Test Split (80% Train, 20% Test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature Scaling (Standardization)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize Linear Regression Model
model = LinearRegression()

# Train the model
model.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test_scaled)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

# Example of making predictions on new data
new_data = {
    'Response Time (sec)': [50],
    'Customer Satisfaction Score': [7],
    'Chatbot Interactions': [40]
}

# Convert new input into DataFrame
new_input = pd.DataFrame(new_data)

# Scale the new input data using the same scaler
new_input_scaled = scaler.transform(new_input)

# Make prediction for new data
predicted_grade = model.predict(new_input_scaled)

print(f"\nPredicted Final Grade for new customer: {predicted_grade[0]:.2f}")

"""Deployment-Building an Interactive App"""

import streamlit as st
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

# Sample dataset for training the model
data = {
    'Response Time (sec)': [34, 120, 60, 90, 45, 30, 100, 70],
    'Customer Satisfaction Score': [7, 8, 5, 9, 6, 7, 8, 6],
    'Chatbot Interactions': [15, 45, 30, 60, 50, 20, 80, 55],
    'Final Grade': [85, 60, 70, 95, 80, 72, 90, 75]  # Target: Final Grade (score)
}

# Convert data to DataFrame
df = pd.DataFrame(data)

# Features (X) and Target (y)
X = df[['Response Time (sec)', 'Customer Satisfaction Score', 'Chatbot Interactions']]
y = df['Final Grade']

# Train-Test Split (80% Train, 20% Test)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature Scaling (Standardization)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train a Linear Regression Model
model = LinearRegression()
model.fit(X_train_scaled, y_train)

# Streamlit App
def app():
    st.title("Customer Satisfaction and Final Grade Prediction")

    # User input for features
    response_time = st.number_input("Response Time (sec)", min_value=0, max_value=300, value=50)
    satisfaction_score = st.slider("Customer Satisfaction Score", min_value=1, max_value=10, value=7)
    chatbot_interactions = st.number_input("Chatbot Interactions", min_value=1, max_value=100, value=30)

    # Prepare the input data
    input_data = pd.DataFrame({
        'Response Time (sec)': [response_time],
        'Customer Satisfaction Score': [satisfaction_score],
        'Chatbot Interactions': [chatbot_interactions]
    })

    # Scale the input data
    input_scaled = scaler.transform(input_data)

    # Make prediction when the user clicks the button
    if st.button('Predict Final Grade'):
        predicted_grade = model.predict(input_scaled)
        st.write(f"Predicted Final Grade: {predicted_grade[0]:.2f}")

    # Optionally show the original data for reference
    if st.checkbox("Show Training Data"):
        st.write(df)

# Run the Streamlit app
if __name__ == "__main__":
    app()

!pip install streamlit

"""Create a Predition Function"""

import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Sample dataset: customer interactions data
data = {
    'Response Time (sec)': [34, 120, 60, 90, 45, 30, 100, 70],
    'Customer Satisfaction Score': [7, 8, 5, 9, 6, 7, 8, 6],
    'Chatbot Interactions': [15, 45, 30, 60, 50, 20, 80, 55],
    'Final Grade': [85, 60, 70, 95, 80, 72, 90, 75]  # Target: Final Grade (score)
}

# Convert to DataFrame
df = pd.DataFrame(data)

# Features (X) and Target (y)
X = df[['Response Time (sec)', 'Customer Satisfaction Score', 'Chatbot Interactions']]
y = df['Final Grade']

# Train-Test Split (80% Train, 20% Test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature Scaling (Standardization)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train the Linear Regression Model
model = LinearRegression()
model.fit(X_train_scaled, y_train)

# Define the Prediction Function
def predict_final_grade(response_time, satisfaction_score, chatbot_interactions):
    # Prepare the input data as a DataFrame
    input_data = pd.DataFrame({
        'Response Time (sec)': [response_time],
        'Customer Satisfaction Score': [satisfaction_score],
        'Chatbot Interactions': [chatbot_interactions]
    })

    # Scale the input data using the same scaler used for training
    input_scaled = scaler.transform(input_data)

    # Make the prediction using the trained model
    predicted_grade = model.predict(input_scaled)

    # Return the predicted final grade
    return predicted_grade[0]

# Example usage of the Prediction Function
response_time = 50
satisfaction_score = 7
chatbot_interactions = 40

predicted_grade = predict_final_grade(response_time, satisfaction_score, chatbot_interactions)

print(f"Predicted Final Grade: {predicted_grade:.2f}")

"""Create the Gradio Interface"""

import gradio as gr
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Sample dataset: customer interactions data
data = {
    'Response Time (sec)': [34, 120, 60, 90, 45, 30, 100, 70],
    'Customer Satisfaction Score': [7, 8, 5, 9, 6, 7, 8, 6],
    'Chatbot Interactions': [15, 45, 30, 60, 50, 20, 80, 55],
    'Final Grade': [85, 60, 70, 95, 80, 72, 90, 75]  # Target: Final Grade (score)
}

# Convert to DataFrame
df = pd.DataFrame(data)

# Features (X) and Target (y)
X = df[['Response Time (sec)', 'Customer Satisfaction Score', 'Chatbot Interactions']]
y = df['Final Grade']

# Train-Test Split (80% Train, 20% Test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature Scaling (Standardization)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train the Linear Regression Model
model = LinearRegression()
model.fit(X_train_scaled, y_train)

# Define the Prediction Function
def predict_final_grade(response_time, satisfaction_score, chatbot_interactions):
    # Prepare the input data as a DataFrame
    input_data = pd.DataFrame({
        'Response Time (sec)': [response_time],
        'Customer Satisfaction Score': [satisfaction_score],
        'Chatbot Interactions': [chatbot_interactions]
    })

    # Scale the input data using the same scaler used for training
    input_scaled = scaler.transform(input_data)

    # Make the prediction using the trained model
    predicted_grade = model.predict(input_scaled)

    # Return the predicted final grade
    return f"Predicted Final Grade: {predicted_grade[0]:.2f}"

# Create Gradio Interface
inputs = [
    gr.Number(label="Response Time (sec)", value=50, precision=0),
    gr.Slider(minimum=1, maximum=10, label="Customer Satisfaction Score", value=7),
    gr.Number(label="Chatbot Interactions", value=40, precision=0)
]

outputs = gr.Textbox()

# Launch Gradio Interface
gr.Interface(fn=predict_final_grade, inputs=inputs, outputs=outputs, live=True).launch()